{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T19:51:27.516129Z",
     "start_time": "2024-04-23T19:51:27.507166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch import nn\n",
    "\n",
    "plt.style.use('ggplot') "
   ],
   "id": "8802c9975469d406",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T19:50:21.619825Z",
     "start_time": "2024-04-23T19:50:21.614711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ],
   "id": "5416a14798200f79",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ebe23148f30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# https://debuggercafe.com/text-classification-using-transformer-encoder-in-pytorch/",
   "id": "4294c69a587d4263"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T19:53:05.469602Z",
     "start_time": "2024-04-23T19:53:05.465759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "OUTPUTS_DIR = '/home/eduardoferreira/PycharmProjects/Rec_Sys/dataset/'\n",
    "os.makedirs(OUTPUTS_DIR, exist_ok=True)\n",
    "data_dir = os.path.join('/home/eduardoferreira/PycharmProjects/Rec_Sys/dataset/aclImdb')\n",
    "dataset_dir = os.path.join(data_dir)\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "print(os.listdir(dataset_dir))\n",
    "print(os.listdir(train_dir))"
   ],
   "id": "a09d78a6168396e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'README', 'imdb.vocab', 'imdbEr.txt', 'test']\n",
      "['urls_neg.txt', 'neg', 'unsupBow.feat', 'labeledBow.feat', 'urls_pos.txt', 'unsup', 'pos', 'urls_unsup.txt']\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T19:53:32.966105Z",
     "start_time": "2024-04-23T19:53:32.963403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_LEN = 1024\n",
    "# Use these many top words from the dataset. If -1, use all words.\n",
    "NUM_WORDS = 32000 # Vocabulary size.\n",
    "# Batch size.\n",
    "BATCH_SIZE = 32\n",
    "VALID_SPLIT = 0.20\n",
    "EPOCHS = 30\n",
    "LR = 0.00001"
   ],
   "id": "500671d61ed00994",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T19:53:57.924814Z",
     "start_time": "2024-04-23T19:53:57.032051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_longest_length(text_file_paths):\n",
    "    \"\"\"\n",
    "    Find the longest review length in the entire training set. \n",
    "    :param text_file_paths: List, containing all the text file paths.\n",
    "    Returns:\n",
    "        max_len: Longest review length.\n",
    "    \"\"\"\n",
    "    max_length = 0\n",
    "    for path in text_file_paths:\n",
    "        with open(path, 'r') as f:\n",
    "            text = f.read()\n",
    "            # Remove <br> tags.\n",
    "            text = re.sub('<[^>]+>+', '', text)\n",
    "            corpus = [\n",
    "                word for word in text.split()\n",
    "            ]\n",
    "        if len(corpus) > max_length:\n",
    "            max_length = len(corpus)\n",
    "    return max_length\n",
    "file_paths = []\n",
    "file_paths.extend(glob.glob(os.path.join(\n",
    "    dataset_dir, 'train', 'pos', '*.txt'\n",
    ")))\n",
    "file_paths.extend(glob.glob(os.path.join(\n",
    "    dataset_dir, 'train', 'neg', '*.txt'\n",
    ")))\n",
    "longest_sentence_length = find_longest_length(file_paths)\n",
    "print(f\"Longest review length: {longest_sentence_length} words\") "
   ],
   "id": "13b35fa1a2f17e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest review length: 2450 words\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T19:55:43.849037Z",
     "start_time": "2024-04-23T19:55:43.845144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_word_frequency(text_file_paths, most_common=None):\n",
    "    \"\"\"\n",
    "    Create a list of tuples of the following format,\n",
    "    [('ho', 2), ('hello', 1), (\"let's\", 1), ('go', 1)]\n",
    "    where the number represents the frequency of occurance of \n",
    "    the word in the entire dataset.\n",
    "    :param text_file_paths: List, containing all the text file paths.\n",
    "    :param most_common: Return these many top words from the dataset.\n",
    "        If `most_common` is None, return all. If `most_common` is 3,\n",
    "        returns the top 3 tuple pairs in the list.\n",
    "    Returns:\n",
    "        sorted_words: A list of tuple containing each word and it's\n",
    "        frequency of the format ('ho', 2), ('hello', 1), ...]\n",
    "    \"\"\"\n",
    "    # Add all the words in the entire dataset to `corpus` list.\n",
    "    corpus = []\n",
    "    for path in text_file_paths:\n",
    "        with open(path, 'r') as f:\n",
    "            text = f.read()\n",
    "            # Remove <br> tags.\n",
    "            text = re.sub('<[^>]+>+', '', text)\n",
    "            corpus.extend([\n",
    "                word for word in text.split()\n",
    "            ])\n",
    "    count_words = Counter(corpus)\n",
    "    # Create a dictionary with the most common word in the corpus \n",
    "    # at the beginning.\n",
    "    # `word_frequency` will be like \n",
    "    word_frequency = count_words.most_common(n=most_common) # Returns all if n is `None`.\n",
    "    return word_frequency"
   ],
   "id": "dc884dcd3b4583ec",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T19:56:46.553766Z",
     "start_time": "2024-04-23T19:56:46.549387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def word2int(input_words, num_words):\n",
    "    \"\"\"\n",
    "    Create a dictionary of word to integer mapping for each unique word.\n",
    "    :param input_words: A list of tuples containing the words and \n",
    "        theiry frequency. Should be of the following format,\n",
    "        [('ho', 2), ('hello', 1), (\"let's\", 1), ('go', 1)]\n",
    "    :param num_words: Number of words to use from the `input_words` list \n",
    "        to create the mapping. If -1, use all words in the dataset.\n",
    "    Returns:\n",
    "        int_mapping: A dictionary of word and a integer mapping as \n",
    "            key-value pair. Example, {'Hello,': 1, 'the': 2, 'let': 3}\n",
    "    \"\"\"\n",
    "    if num_words > -1:\n",
    "        int_mapping = {\n",
    "            w:i+1 for i, (w, c) in enumerate(input_words) \\\n",
    "                if i <= num_words - 1 # -1 to avoid getting (num_words + 1) integer mapping.\n",
    "        }\n",
    "    else:\n",
    "        int_mapping = {w:i+1 for i, (w, c) in enumerate(input_words)}\n",
    "    return int_mapping"
   ],
   "id": "c7fb72409a24c467",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T19:57:03.021526Z",
     "start_time": "2024-04-23T19:57:03.014532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NLPClassificationDataset(Dataset):\n",
    "    def __init__(self, file_paths, word_frequency, int_mapping, max_len):\n",
    "        self.word_frequency = word_frequency\n",
    "        self.int_mapping = int_mapping\n",
    "        self.file_paths = file_paths\n",
    "        self.max_len = max_len\n",
    "    def standardize_text(self, input_text):\n",
    "        # Convert everything to lower case.\n",
    "        text = input_text.lower()\n",
    "        # If the text contains HTML tags, remove them.\n",
    "        text = re.sub('<[^>]+>+', '', text)\n",
    "        # Remove punctuation marks using `string` module.\n",
    "        # According to `string`, the following will be removed,\n",
    "        # '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "        text = ''.join([\n",
    "            character for character in text \\\n",
    "                if character not in string.punctuation\n",
    "        ])\n",
    "        return text\n",
    "    def return_int_vector(self, int_mapping, text_file_path):\n",
    "        \"\"\"\n",
    "        Assign an integer to each word and return the integers in a list.\n",
    "        \"\"\"\n",
    "        with open(text_file_path, 'r') as f:\n",
    "            text = f.read()\n",
    "            text = self.standardize_text(text)\n",
    "            corpus = [\n",
    "                word for word in text.split()\n",
    "            ] \n",
    "        # Each word is replaced by a specific integer.\n",
    "        int_vector = [\n",
    "            int_mapping[word] for word in text.split() \\\n",
    "            if word in int_mapping\n",
    "        ]\n",
    "        return int_vector\n",
    "    \n",
    "    def pad_features(self, int_vector, max_len):\n",
    "        \"\"\"\n",
    "        Return features of `int_vector`, where each vector is padded \n",
    "        with 0's or truncated to the input seq_length. Return as Numpy \n",
    "        array.\n",
    "        \"\"\"\n",
    "        features = np.zeros((1, max_len), dtype = int)\n",
    "        if len(int_vector) <= max_len:\n",
    "            zeros = list(np.zeros(max_len - len(int_vector)))\n",
    "            new = zeros + int_vector\n",
    "        else:\n",
    "            new = int_vector[: max_len]\n",
    "        features = np.array(new)\n",
    "        return features\n",
    "    def encode_labels(self, text_file_path):\n",
    "        file_path = pathlib.Path(text_file_path)\n",
    "        class_label = str(file_path).split(os.path.sep)[-2]\n",
    "        if class_label == 'pos':\n",
    "            int_label = 1\n",
    "        else:\n",
    "            int_label = 0\n",
    "        return int_label\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        int_vector = self.return_int_vector(self.int_mapping, file_path)\n",
    "        padded_features = self.pad_features(int_vector, self.max_len)\n",
    "        label = self.encode_labels(file_path)\n",
    "        return {\n",
    "            'text': torch.tensor(padded_features, dtype=torch.int32),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ],
   "id": "a701d9546ecbfa03",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T19:57:23.805276Z",
     "start_time": "2024-04-23T19:57:23.727426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# List of all file paths.\n",
    "file_paths = []\n",
    "file_paths.extend(glob.glob(os.path.join(\n",
    "    dataset_dir, 'train', 'pos', '*.txt'\n",
    ")))\n",
    "file_paths.extend(glob.glob(os.path.join(\n",
    "    dataset_dir, 'train', 'neg', '*.txt'\n",
    ")))\n",
    "test_file_paths = []\n",
    "test_file_paths.extend(glob.glob(os.path.join(\n",
    "    dataset_dir, 'test', 'pos', '*.txt'\n",
    ")))\n",
    "test_file_paths.extend(glob.glob(os.path.join(\n",
    "    dataset_dir, 'test', 'neg', '*.txt'\n",
    ")))"
   ],
   "id": "d6845ebfd15870e9",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T19:57:31.398632Z",
     "start_time": "2024-04-23T19:57:29.707916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the frequency of all unqiue words in the dataset.\n",
    "word_frequency = find_word_frequency(file_paths)\n",
    "# Assign a specific intenger to each word.\n",
    "int_mapping = word2int(word_frequency, num_words=NUM_WORDS)"
   ],
   "id": "34d70e4578bef102",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T19:57:36.920257Z",
     "start_time": "2024-04-23T19:57:36.913193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = NLPClassificationDataset(\n",
    "    file_paths, word_frequency, int_mapping, MAX_LEN\n",
    ")\n",
    "dataset_size = len(dataset)\n",
    "# Calculate the validation dataset size.\n",
    "valid_size = int(VALID_SPLIT*dataset_size)\n",
    "# Radomize the data indices.\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "# Training and validation sets.\n",
    "dataset_train = Subset(dataset, indices[:-valid_size])\n",
    "dataset_valid = Subset(dataset, indices[-valid_size:])\n",
    "dataset_test = NLPClassificationDataset(\n",
    "    test_file_paths, word_frequency, int_mapping, MAX_LEN\n",
    ")\n",
    "# dataset_valid = NLPClassificationDataset()\n",
    "print(f\"Number of training samples: {len(dataset_train)}\")\n",
    "print(f\"Number of validation samples: {len(dataset_valid)}\")\n",
    "print(f\"Number of test samples: {len(dataset_test)}\")"
   ],
   "id": "5abc5bde52d91d80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 20000\n",
      "Number of validation samples: 5000\n",
      "Number of test samples: 24995\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T19:57:48.179069Z",
     "start_time": "2024-04-23T19:57:48.174353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset_train, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True, \n",
    "    num_workers=4\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    dataset_valid, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, \n",
    "    num_workers=4\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset_test, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")"
   ],
   "id": "233bc42cdf2f4dc6",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T19:57:53.198924Z",
     "start_time": "2024-04-23T19:57:53.195202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_correct_incorrect(labels, outputs, train_running_correct):\n",
    "    # As the outputs are currently logits.\n",
    "    outputs = torch.sigmoid(outputs)\n",
    "    running_correct = 0\n",
    "    for i, label in enumerate(labels):\n",
    "        if label < 0.5 and outputs[i] < 0.5:\n",
    "            running_correct += 1\n",
    "        elif label >= 0.5 and outputs[i] >= 0.5:\n",
    "            running_correct += 1\n",
    "    return running_correct"
   ],
   "id": "365e897c70f595b1",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T19:57:59.959630Z",
     "start_time": "2024-04-23T19:57:59.953169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training function.\n",
    "def train(model, trainloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    print('Training')\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    counter = 0\n",
    "    for i, data in tqdm(enumerate(trainloader), total=len(trainloader)):\n",
    "        counter += 1\n",
    "        inputs, labels = data['text'], data['label']\n",
    "        inputs = inputs.to(device)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass.\n",
    "        outputs = model(inputs)\n",
    "        outputs = torch.squeeze(outputs, -1)\n",
    "        # Calculate the loss.\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_running_loss += loss.item()\n",
    "        running_correct = count_correct_incorrect(\n",
    "            labels, outputs, train_running_correct\n",
    "        )\n",
    "        train_running_correct += running_correct\n",
    "        # Backpropagation.\n",
    "        loss.backward()\n",
    "        # Update the optimizer parameters.\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Loss and accuracy for the complete epoch.\n",
    "    epoch_loss = train_running_loss / counter\n",
    "    epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n",
    "    return epoch_loss, epoch_acc\n",
    "# Validation function.\n",
    "def validate(model, testloader, criterion, device):\n",
    "    model.eval()\n",
    "    print('Validation')\n",
    "    valid_running_loss = 0.0\n",
    "    valid_running_correct = 0\n",
    "    counter = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(testloader), total=len(testloader)):\n",
    "            counter += 1\n",
    "            inputs, labels = data['text'], data['label']\n",
    "            inputs = inputs.to(device)\n",
    "            labels = torch.tensor(labels, dtype=torch.float32).to(device)\n",
    "            # Forward pass.\n",
    "            outputs = model(inputs)\n",
    "            outputs = torch.squeeze(outputs, -1)\n",
    "            # Calculate the loss.\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_running_loss += loss.item()\n",
    "            running_correct = count_correct_incorrect(\n",
    "                labels, outputs, valid_running_correct\n",
    "            )\n",
    "            valid_running_correct += running_correct\n",
    "        \n",
    "    # Loss and accuracy for the complete epoch.\n",
    "    epoch_loss = valid_running_loss / counter\n",
    "    epoch_acc = 100. * (valid_running_correct / len(testloader.dataset))\n",
    "    return epoch_loss, epoch_acc"
   ],
   "id": "609e280fd6d42856",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T19:58:07.675226Z",
     "start_time": "2024-04-23T19:58:07.672927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model parameters.\n",
    "EMBED_DIM = 256\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_HEADS = 4"
   ],
   "id": "65a23731f2ce60e2",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T19:58:14.489310Z",
     "start_time": "2024-04-23T19:58:14.482037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers, num_heads):\n",
    "        super(EncoderClassifier, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=self.encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.linear = nn.Linear(embed_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.max(dim=1)[0]\n",
    "        out = self.linear(x)\n",
    "        return out  "
   ],
   "id": "3d871b5984429efa",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T19:58:20.544048Z",
     "start_time": "2024-04-23T19:58:20.481455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = EncoderClassifier(\n",
    "    len(int_mapping)+1, \n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_layers=NUM_ENCODER_LAYERS,\n",
    "    num_heads=NUM_HEADS\n",
    ").to(device)\n",
    "print(model)\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\\n\")"
   ],
   "id": "8ea4359d27504bcb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderClassifier(\n",
      "  (emb): Embedding(32001, 256)\n",
      "  (encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "13,452,801 total parameters.\n",
      "13,452,801 training parameters.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T19:58:39.852136Z",
     "start_time": "2024-04-23T19:58:39.300136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=LR,\n",
    ")"
   ],
   "id": "bdda04e639ff66e3",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-23T19:58:46.127956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Lists to keep track of losses and accuracies.\n",
    "train_loss, valid_loss = [], []\n",
    "train_acc, valid_acc = [], []\n",
    "least_loss = float('inf')\n",
    "# Start the training.\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"[INFO]: Epoch {epoch+1} of {EPOCHS}\")\n",
    "    train_epoch_loss, train_epoch_acc = train(model, train_loader, \n",
    "                                            optimizer, criterion, device)\n",
    "    valid_epoch_loss, valid_epoch_acc = validate(model, valid_loader,  \n",
    "                                                criterion, device)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    valid_loss.append(valid_epoch_loss)\n",
    "    train_acc.append(train_epoch_acc)\n",
    "    valid_acc.append(valid_epoch_acc)\n",
    "    print(f\"Training loss: {train_epoch_loss}, training acc: {train_epoch_acc}\")\n",
    "    print(f\"Validation loss: {valid_epoch_loss}, validation acc: {valid_epoch_acc}\")\n",
    "    # Save model.\n",
    "    if valid_epoch_loss < least_loss:\n",
    "        least_loss = valid_epoch_loss\n",
    "        print(f\"Saving best model till now... LEAST LOSS {valid_epoch_loss:.3f}\")\n",
    "        torch.save(\n",
    "            model, os.path.join(OUTPUTS_DIR, 'model.pth')\n",
    "        )\n",
    "    print('-'*50)"
   ],
   "id": "1d7b8e18d753036f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Epoch 1 of 30\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d7f1aaaf5e9407585f3e11b3ec10ab2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8262/618881986.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bfdfe551b13f44ffbb09d0d4e752858d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8262/618881986.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6355689167499542, training acc: 65.365\n",
      "Validation loss: 0.5613470326183708, validation acc: 76.44\n",
      "Saving best model till now... LEAST LOSS 0.561\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 2 of 30\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "80b0e4c7f21b4b28b5af4c6b6430a8ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f9134073cf2e41bea564a69637c6560a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5102215754032136, training acc: 77.12\n",
      "Validation loss: 0.4809391122714729, validation acc: 79.72\n",
      "Saving best model till now... LEAST LOSS 0.481\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 3 of 30\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f2afd8548bc40c1aca72794d236d656"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "56323ab747147fe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
